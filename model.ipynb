{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.utils import to_categorical\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downsample_block_win_smaller_wout(input_tensor, block_number, in_channels, out_channels):\n",
    "    \n",
    "    conv_filters = out_channels - in_channels\n",
    "    \n",
    "    with tf.variable_scope('downsample_block_' + str(block_number), reuse=tf.AUTO_REUSE) as scope:\n",
    "        weights = tf.get_variable('weights',\n",
    "                                  shape=[3, 3, in_channels, conv_filters],\n",
    "                                  dtype=tf.float32)\n",
    "        biases = tf.get_variable('biases',\n",
    "                                 shape=[conv_filters],\n",
    "                                 dtype=tf.float32)\n",
    "        pre_conv = tf.nn.conv2d(input_tensor, weights, strides=[1, 2, 2, 1], padding='SAME')\n",
    "        conv = tf.nn.bias_add(pre_conv, biases)\n",
    "        \n",
    "        max_pool= tf.nn.max_pool(input_tensor, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME', name='pooling')\n",
    "        \n",
    "        concat = tf.concat([conv, max_pool], axis=-1)\n",
    "        \n",
    "        with tf.variable_scope('bn_relu', reuse=tf.AUTO_REUSE) as scope:\n",
    "            bn = tf.layers.batch_normalization(concat)\n",
    "            relu = tf.nn.relu(bn)\n",
    "            out = relu\n",
    "        \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downsample_block_win_greater_wout(input_tensor, block_number, in_channels, out_channels):\n",
    "    \n",
    "    conv_filters = out_channels\n",
    "    \n",
    "    with tf.variable_scope('downsample_block_'+ str(block_number), reuse=tf.AUTO_REUSE) as scope:\n",
    "        weights = tf.get_variable('weights',\n",
    "                                  shape=[3, 3, in_channels, conv_filters],\n",
    "                                  dtype=tf.float32)\n",
    "        biases = tf.get_variable('biases',\n",
    "                                 shape=[conv_filters],\n",
    "                                 dtype=tf.float32)\n",
    "        pre_conv = tf.nn.conv2d(input_tensor, weights, strides=[1, 2, 2, 1], padding='SAME')\n",
    "        conv = tf.nn.bias_add(pre_conv, biases)\n",
    "        \n",
    "        with tf.variable_scope('bn_relu', reuse=tf.AUTO_REUSE) as scope:\n",
    "            bn = tf.layers.batch_normalization(conv)\n",
    "            relu = tf.nn.relu(bn)\n",
    "            out = relu\n",
    "    return out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def asym_block(input_tensor, inchannels, outchannels, dilation=1):\n",
    "    \n",
    "    if dilation>1:\n",
    "        dilation_name = '_d'\n",
    "    else:\n",
    "        dilation_name = ''\n",
    "    \n",
    "    with tf.variable_scope('asym_conv' + dilation_name, reuse=tf.AUTO_REUSE) as scope:\n",
    "\n",
    "        with tf.variable_scope('asym_3x1', reuse=tf.AUTO_REUSE) as scope:\n",
    "\n",
    "            weights = tf.get_variable('weights',\n",
    "                                      shape=[3, 1, inchannels, outchannels],\n",
    "                                      dtype=tf.float32)\n",
    "            biases = tf.get_variable('biases',\n",
    "                                     shape=[outchannels],\n",
    "                                     dtype=tf.float32)\n",
    "            pre_conv = tf.nn.conv2d(input_tensor, weights, strides=[1, 1, 1, 1], dilations=[1, dilation, dilation, 1], padding='SAME')\n",
    "            conv_asym_3x1 = tf.nn.bias_add(pre_conv, biases)\n",
    "\n",
    "        with tf.variable_scope('asym_1x3', reuse=tf.AUTO_REUSE) as scope:\n",
    "\n",
    "            weights = tf.get_variable('weights',\n",
    "                                      shape=[1, 3, inchannels, outchannels],\n",
    "                                      dtype=tf.float32)\n",
    "            biases = tf.get_variable('biases',\n",
    "                                     shape=[outchannels],\n",
    "                                     dtype=tf.float32)\n",
    "            pre_conv = tf.nn.conv2d(conv_asym_3x1, weights, strides=[1, 1, 1, 1], dilations=[1, dilation, dilation, 1], padding='SAME')\n",
    "            conv_asym_1x3 = tf.nn.bias_add(pre_conv, biases)\n",
    "            \n",
    "        with tf.variable_scope('bn_relu', reuse=tf.AUTO_REUSE) as scope:\n",
    "            \n",
    "            bn = tf.layers.batch_normalization(conv_asym_1x3)\n",
    "            relu = tf.nn.relu(bn)\n",
    "        out = relu\n",
    "    return out\n",
    "\n",
    "def pointwise_conv(input_tensor, inchannels, outchannels):\n",
    "    with tf.variable_scope('1x1_conv', reuse=tf.AUTO_REUSE) as scope:\n",
    "        weights = tf.get_variable('weights',\n",
    "                                  shape=[1, 1, inchannels, outchannels],\n",
    "                                  dtype=tf.float32)\n",
    "        biases = tf.get_variable('biases',\n",
    "                                 shape=[outchannels],\n",
    "                                 dtype=tf.float32)\n",
    "        pre_conv = tf.nn.conv2d(input_tensor, weights, strides=[1, 1, 1, 1], padding='SAME')\n",
    "        conv_1x1 = tf.nn.bias_add(pre_conv, biases)\n",
    "        bn_1x1 = tf.layers.batch_normalization(pre_conv)\n",
    "        relu_1x1 = tf.nn.relu(bn_1x1)\n",
    "        out_1x1 = relu_1x1\n",
    "    return out_1x1\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eda_block(input_tensor, module_number, block_number, inchannels, growth_rate, dilation_rate):\n",
    "    \n",
    "    with tf.variable_scope('EDAblock_' + str(module_number) + '_' +  str(block_number), reuse=tf.AUTO_REUSE) as scope:\n",
    "\n",
    "        conv_1x1 = pointwise_conv(input_tensor, inchannels, growth_rate)\n",
    "\n",
    "        asym_1 = asym_block(conv_1x1, growth_rate, growth_rate)\n",
    "\n",
    "        asym_2_d = asym_block(asym_1, growth_rate, growth_rate, dilation=dilation_rate)\n",
    "\n",
    "        keep_prob = tf.constant([0.98], dtype=tf.float32,shape=())\n",
    "        dropout = tf.nn.dropout(asym_2_d, keep_prob=keep_prob)\n",
    "        \n",
    "        concat = tf.concat([input_tensor, dropout], axis=-1)\n",
    "#         print(concat)\n",
    "    return concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_eda_module_1(input_tensor, growth_rate, inchannels):\n",
    "    with tf.variable_scope('EDAmodule_1', reuse=tf.AUTO_REUSE) as scope:\n",
    "        eda_block_1_1 = eda_block(input_tensor, 1, 1, inchannels, growth_rate, 1)\n",
    "        eda_block_1_2 = eda_block(eda_block_1_1, 1, 2, 100, growth_rate, 1)\n",
    "        eda_block_1_3 = eda_block(eda_block_1_2, 1, 3, 140, growth_rate, 1)\n",
    "        eda_block_1_4 = eda_block(eda_block_1_3, 1, 4, 180, growth_rate, 2)\n",
    "        eda_block_1_4 = eda_block(eda_block_1_4, 1, 5, 220, growth_rate, 2)\n",
    "    return eda_block_1_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_eda_module_2(input_tensor, growth_rate, inchannels):\n",
    "    with tf.variable_scope('EDAmodule_2', reuse=tf.AUTO_REUSE) as scope:\n",
    "        eda_block_2_1 = eda_block(input_tensor, 2, 1, inchannels, growth_rate, 2)\n",
    "        eda_block_2_2 = eda_block(eda_block_2_1, 2, 2, 170, growth_rate, 2)\n",
    "        eda_block_2_3 = eda_block(eda_block_2_2, 2, 3, 210, growth_rate, 4)\n",
    "        eda_block_2_4 = eda_block(eda_block_2_3, 2, 4, 250, growth_rate, 4)\n",
    "        eda_block_2_5 = eda_block(eda_block_2_4, 2, 5, 290, growth_rate, 8)\n",
    "        eda_block_2_6 = eda_block(eda_block_2_5, 2, 6, 330, growth_rate, 8)\n",
    "        eda_block_2_7 = eda_block(eda_block_2_6, 2, 7, 370, growth_rate, 16)\n",
    "        eda_block_2_8 = eda_block(eda_block_2_7, 2, 8, 410, growth_rate, 16)\n",
    "    return eda_block_2_8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def projection_layer(input_tensor, inchannels, outchannels):\n",
    "    \n",
    "    with tf.variable_scope('Projection_Layer', reuse=tf.AUTO_REUSE) as scope:\n",
    "        weights = tf.get_variable('weights',\n",
    "                                  shape=[1, 1, inchannels, outchannels],\n",
    "                                  dtype=tf.float32)\n",
    "        biases = tf.get_variable('biases',\n",
    "                                 shape=[outchannels],\n",
    "                                 dtype=tf.float32)\n",
    "        pre_conv = tf.nn.conv2d(input_tensor, weights, strides=[1, 1, 1, 1], padding='SAME')\n",
    "        projection_1x1 = tf.nn.bias_add(pre_conv, biases)\n",
    "    return projection_1x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upsample_image(input_tensor):\n",
    "    with tf.variable_scope('Upsampling_8x') as scope:\n",
    "        resized_image = tf.image.resize_images(input_tensor, (512, 1024))\n",
    "    return resized_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(input_image, num_classes):\n",
    "    \n",
    "    with tf.variable_scope('Preprocess', reuse=tf.AUTO_REUSE) as scope:\n",
    "        input_resized_image = tf.image.resize_images(input_image, (512, 1024))\n",
    "        print('Input Image : ', input_resized_image)\n",
    "    with tf.variable_scope('Model', reuse=tf.AUTO_REUSE) as scope:\n",
    "        downsample1 = downsample_block_win_smaller_wout(input_resized_image, 1, 3, 15)\n",
    "        downsample2 = downsample_block_win_smaller_wout(downsample1, 2, 15, 60)\n",
    "        eda_module_1 = get_eda_module_1(downsample2, 40, 60)\n",
    "        downsample3 = downsample_block_win_greater_wout(eda_module_1, 3, 260, 130)\n",
    "        eda_module_2 = get_eda_module_2(downsample3, 40, 130)\n",
    "        projection = projection_layer(eda_module_2, 450, num_classes)\n",
    "    with tf.variable_scope('Postprocess', reuse=tf.AUTO_REUSE) as scope:\n",
    "        resized_image = upsample_image(projection)\n",
    "        resized_image = tf.argmax(resized_image, axis=-1)\n",
    "    return projection, resized_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss(logits, labels):\n",
    "    with tf.variable_scope('Loss', reuse=tf.AUTO_REUSE) as scope:\n",
    "        resized_labels = tf.image.resize_images(labels, (64, 128))\n",
    "        resized_labels = tf.dtypes.cast(resized_labels, dtype=tf.int32)\n",
    "        encoded_labels = tf.one_hot(resized_labels, axis=-1, depth=40)\n",
    "        encoded_labels = tf.reshape(encoded_labels, shape=[-1, 64, 128, 40])\n",
    "        print('Labels : ', encoded_labels)\n",
    "        print('Logits : ', logits)\n",
    "        cross_entropy = tf.nn.softmax_cross_entropy_with_logits\\\n",
    "                        (logits=logits, labels=encoded_labels, name='xentropy_per_example')\n",
    "        loss = tf.reduce_mean(cross_entropy, name='loss')\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_op(loss, learning_rate):\n",
    "    with tf.variable_scope('Optimizer', reuse=tf.AUTO_REUSE) as scope:\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate= learning_rate)\n",
    "        train_op = optimizer.minimize(loss)\n",
    "    return train_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Image :  Tensor(\"Preprocess/resize_images/ResizeBilinear:0\", shape=(?, 512, 1024, 3), dtype=float32)\n",
      "Labels :  Tensor(\"Loss/Reshape:0\", shape=(?, 64, 128, 40), dtype=float32)\n",
      "Logits :  Tensor(\"Model/Projection_Layer/BiasAdd:0\", shape=(?, 64, 128, 40), dtype=float32)\n",
      "WARNING:tensorflow:From <ipython-input-11-c8d4bb77a6ca>:9: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "Model Output :  Tensor(\"Model/Projection_Layer/BiasAdd:0\", shape=(?, 64, 128, 40), dtype=float32)\n",
      "Postprocessed Output :  Tensor(\"Postprocess/ArgMax:0\", shape=(?, 512, 1024), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "num_classes = 40\n",
    "input_image = tf.placeholder(shape=(None, None, None, 3), dtype=tf.float32)\n",
    "input_labels = tf.placeholder(shape=(None, None, None, 1), dtype=tf.float32)\n",
    "learning_rate = tf.placeholder(shape=(), dtype=tf.float32)\n",
    "\n",
    "model_out, resized_model_out = build_model(input_image, num_classes)\n",
    "model_loss = get_loss(model_out, input_labels)\n",
    "\n",
    "train_op = get_train_op(model_loss, learning_rate)\n",
    "\n",
    "print('Model Output : ', model_out)\n",
    "print('Postprocessed Output : ', resized_model_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    writer = tf.summary.FileWriter('./logs/', sess.graph)\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = [], []\n",
    "for i in os.listdir('./../preperation/IDD/Train/Images/')[:1]:\n",
    "    images.append(cv2.imread('./../preperation/IDD/Train/Images/' + i))\n",
    "for i in os.listdir('./../preperation/IDD/Train/Annotations/')[:1]:\n",
    "    labels.append(cv2.imread('./../preperation/IDD/Train/Annotations/' + i)[:, :, 0])\n",
    "images = np.array(images, dtype='float32')\n",
    "labels = np.array(labels, dtype='float32').reshape((1, 1080, 1920, 1))\n",
    "images.shape, labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    _, loss = sess.run([train_op, model_loss], feed_dict={input_image:images, input_labels:labels})\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
